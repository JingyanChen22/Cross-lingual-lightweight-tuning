{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["wnFgGs05RhJy","U4j5bk-oRntY","Ok6p2lNXMG8k","Tx82HX2l4Un5","2YoY5WXc8HGp"],"toc_visible":true,"authorship_tag":"ABX9TyOIHjYIAjHStS4C7CRkzLe/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1zzmQJCQOKljRdTMu7vyXZ7q5Iomt5uN9?usp=sharing)"],"metadata":{"id":"nqrxr_TtAfTm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4nQHIzu73ub4"},"outputs":[],"source":["!pip install -U adapter-transformers\n","!pip install -U datasets"]},{"cell_type":"markdown","source":["# model and tokenizer"],"metadata":{"id":"wnFgGs05RhJy"}},{"cell_type":"code","source":["from transformers import AutoConfig, AutoModelForQuestionAnswering, AutoTokenizer\n","import torch\n","model_name = \"xlm-roberta-base\"\n","config = AutoConfig.from_pretrained(model_name)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = AutoModelForQuestionAnswering.from_pretrained(model_name, config=config).to(device)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"],"metadata":{"id":"sRBp0psO7uFd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Without adapter/prefix parameters to train:\")\n","print(sum(p.numel() for p in model.parameters() if p.requires_grad))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uEXeF2PsSF0i","executionInfo":{"status":"ok","timestamp":1678754306162,"user_tz":-60,"elapsed":24,"user":{"displayName":"Jingyan Chen","userId":"08159604947673382978"}},"outputId":"02c9573b-6b6a-4232-c626-74056e730fc1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Without adapter parameters to train:\n","277454594\n"]}]},{"cell_type":"markdown","source":["# data processing"],"metadata":{"id":"U4j5bk-oRntY"}},{"cell_type":"code","source":["from datasets import load_dataset\n","import datasets\n","\n","dataset = load_dataset(\"squad\")\n","dataset[\"train\"] = dataset[\"train\"].shuffle(seed=42).select(range(50000))\n","dataset[\"validation\"] = dataset[\"validation\"].shuffle(seed=42).select(range(6250))"],"metadata":{"id":"g1n2JcMoRWFK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_length = 384\n","doc_stride = 128\n","\n","def tokenize_sample_data(data):\n","  tokenized_feature = tokenizer(\n","    data[\"question\"],\n","    data[\"context\"],\n","    max_length = max_length,\n","    return_overflowing_tokens=True,\n","    stride=doc_stride,\n","    truncation=\"only_second\",\n","    padding = \"max_length\",\n","    return_offsets_mapping=True, \n","  )\n","\n","\n","  sample_mapping = tokenized_feature.pop(\"overflow_to_sample_mapping\")\n","  offset_mapping = tokenized_feature.pop(\"offset_mapping\")\n","\n","  tokenized_feature[\"start_positions\"] = []\n","  tokenized_feature[\"end_positions\"] = []\n","  for i, offsets in enumerate(offset_mapping):\n","\n","\n","    sample_index = sample_mapping[i]\n","    answers = data[\"answers\"][sample_index]\n","\n","    input_ids = tokenized_feature[\"input_ids\"][i]\n","    cls_index = input_ids.index(tokenizer.cls_token_id)\n","\n","    sequence_ids = tokenized_feature.sequence_ids(i)\n","\n","    if len(answers[\"answer_start\"]) == 0:\n","      tokenized_feature[\"start_positions\"].append(cls_index)\n","      tokenized_feature[\"end_positions\"].append(cls_index)\n","    else:\n","      start_char = answers[\"answer_start\"][0]\n","      end_char = start_char + len(answers[\"text\"][0])\n","\n","\n","      idx = 0\n","      while sequence_ids[idx] != 1:\n","        idx += 1\n","      context_start = idx\n","      context_end = len(input_ids) - 1\n","      while sequence_ids[context_end] != 1:\n","        context_end -= 1\n","\n","      if not (\n","          offsets[context_start][0] <= start_char\n","          and offsets[context_end][1] >= end_char\n","      ):\n","        tokenized_feature[\"start_positions\"].append(cls_index)\n","        tokenized_feature[\"end_positions\"].append(cls_index)\n","      else:\n","        while (\n","            context_start < len(offsets)\n","            and offsets[context_start][0] <= start_char\n","        ):\n","            context_start += 1\n","        tokenized_feature[\"start_positions\"].append(context_start - 1)\n","        while offsets[context_end][1] >= end_char:\n","            context_end -= 1\n","        tokenized_feature[\"end_positions\"].append(context_end + 1)\n","  return tokenized_feature\n","\n","tokenized_ds = dataset.map(\n","  tokenize_sample_data,\n","  remove_columns=dataset[\"train\"].column_names,\n","  batched=True,\n","  batch_size=128)"],"metadata":{"id":"gBD7nSTRRy0W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# set adapters/prefix"],"metadata":{"id":"vrroceim48hs"}},{"cell_type":"markdown","source":["## adapters"],"metadata":{"id":"Ok6p2lNXMG8k"}},{"cell_type":"code","source":["# language adapter\n","from transformers import AdapterConfig\n","from transformers.adapters.composition import Stack\n","lang_adapter_config = AdapterConfig.load(\"pfeiffer\", reduction_factor=2)\n","model.load_adapter(\"en/wiki@ukp\", config=lang_adapter_config)\n","\n","adap_name = \"qa_squad\"\n","model.add_adapter(adap_name)\n","model.train_adapter(adap_name)\n","\n","# model.set_active_adapters(adap_name)\n","# model.active_adapters = adap_name\n","model.active_adapters = Stack(\"en\",adap_name)"],"metadata":{"id":"Erp5KxueVLHs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"With adapter parameters to train:\")\n","print(sum(p.numel() for p in model.parameters() if p.requires_grad))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9KhL08VVWEc5","executionInfo":{"status":"ok","timestamp":1678754493367,"user_tz":-60,"elapsed":570,"user":{"displayName":"Jingyan Chen","userId":"08159604947673382978"}},"outputId":"0562cba2-0817-4687-ec3a-260db6032f7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["With adapter parameters to train:\n","896066\n"]}]},{"cell_type":"markdown","source":["## prefix"],"metadata":{"id":"Tx82HX2l4Un5"}},{"cell_type":"code","source":["from transformers.adapters import PrefixTuningConfig\n","\n","config = PrefixTuningConfig(flat=False, prefix_length=30)\n","model.add_adapter(\"prefix_tuning\", config=config)\n","\n","model.train_adapter(\"prefix_tuning\")\n","model.active_adapters = \"prefix_tuning\""],"metadata":{"id":"Qgvg5-lx4T_X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"With prefix parameters to train:\")\n","print(sum(p.numel() for p in model.parameters() if p.requires_grad))"],"metadata":{"id":"hmgCPRhb4fXv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# training"],"metadata":{"id":"vZ9idZptMJ1J"}},{"cell_type":"code","source":["from transformers import TrainingArguments,AdapterTrainer,Trainer\n","from transformers import DefaultDataCollator\n","\n","data_collator = DefaultDataCollator()\n","\n","batch_size = 16\n","\n","training_args = TrainingArguments(\n","  output_dir = \"./training_qa\",\n","  log_level = \"error\",\n","  num_train_epochs = 3,\n","  learning_rate = 7e-5,\n","  lr_scheduler_type = \"linear\",\n","  warmup_steps = 100,\n","  per_device_train_batch_size = batch_size,\n","  per_device_eval_batch_size = batch_size,\n","  evaluation_strategy = \"steps\",\n","  eval_steps = 1000, \n","  save_steps = 5000, \n","  logging_steps = 100,\n","  push_to_hub = False\n",")\n","\n","\n","trainer = AdapterTrainer( #using ``Trainer`` for full fine-tuning\n","  model = model,\n","  args = training_args,\n","  data_collator = data_collator,\n","  train_dataset = tokenized_ds[\"train\"],\n","  eval_dataset = tokenized_ds[\"validation\"],\n","  tokenizer = tokenizer,\n",")\n","\n"],"metadata":{"id":"ccfnRQOHKt_v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"jRnseH7Ubl8m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save lightweight tuned model\n","save_path = \"./QA_adapter\" #or \"./QA_prefix\"\n","os.makedirs(save_path, exist_ok=True)\n","trainer.save_model(save_path) "],"metadata":{"id":"zXMYz_3U5ICz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save full fine-tuned model\n","save_path = \"./QA_finetune\"\n","os.makedirs(save_path, exist_ok=True)\n","trainer.model.save_pretrained(save_path)"],"metadata":{"id":"oVUqWNUX6qwz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# evaluation"],"metadata":{"id":"0DSldo8Y8Fkx"}},{"cell_type":"markdown","source":["## load trained model"],"metadata":{"id":"2YoY5WXc8HGp"}},{"cell_type":"code","source":["# full fine-tune\n","from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n","from transformers import pipeline\n","import torch\n","from datasets import load_dataset\n","\n","model_name = \"xlm-roberta-base\"\n","model_path = \"./QA_finetune\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = AutoModelForQuestionAnswering.from_pretrained(model_path).to(device)\n","qa_ppl = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)"],"metadata":{"id":"N2oQmur5800D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# adapters\n","from transformers import AutoTokenizer, AutoModelForQuestionAnswering,AdapterConfig\n","from transformers import pipeline\n","import torch\n","from datasets import load_dataset\n","from transformers.adapters.composition import Stack\n","\n","model_name = \"xlm-roberta-base\"\n","model_path = \"./QA_adapter\"\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(device)\n","\n","model.load_adapter(f\"{model_path}/qa_squad\")\n","\n","lang_adapter_config = AdapterConfig.load(\"pfeiffer\", reduction_factor=2)\n","model.load_adapter(\"en/wiki@ukp\", config=lang_adapter_config)\n","model.active_adapters = Stack(\"en\",\"qa_squad\")"],"metadata":{"id":"NPsIXMuN8-St"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prefix\n","from transformers import AutoConfig, AutoModelForQuestionAnswering, AutoTokenizer\n","import torch\n","model_name = \"xlm-roberta-base\"\n","config = AutoConfig.from_pretrained(model_name)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = AutoModelForQuestionAnswering.from_pretrained(model_name, config=config).to(device)\n","\n","model_path = \"./QA_prefix\"\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","\n","model.load_adapter(f\"{model_path}/prefix_tuning\")\n","model.active_adapters = \"prefix_tuning\""],"metadata":{"id":"_t2C2S_j9H6N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## output result"],"metadata":{"id":"9QrLl8DJ8LUR"}},{"cell_type":"code","source":["import string, re, os\n","import pandas as pd\n","\n","\n","def normaliza_text(text):\n","  \"\"\"\n","  Removing articles and punctuations, and standardizing whitespace.\n","  \"\"\"\n","  text = text.lower()\n","  #remove puncts\n","  exclude = set(string.punctuation) #set of all puncts\n","  text = \"\".join(character for character in text if character not in exclude) #iterate every character\n","  #remove articles\n","  regex = re.compile(r\"\\b(a|an|the)\\b\",re.UNICODE) #build the pattern object， regular experession\n","  text = re.sub(regex,\" \", text)\n","  #fix the whitespece\n","  text = \" \".join(text.split()) #split default by blank\n","  return text\n","\n","def metric_exact_match(prediction,label):\n","  if len(prediction) == 0 or len(label) == 0:\n","    return int(prediction == label)\n","  return int(normaliza_text(prediction) == normaliza_text(label))\n","\n","def metric_f1(prediction,label):\n","  pred_tokens = normaliza_text(prediction).split()\n","  label_tokens = normaliza_text(label).split()\n","\n","  #if either prediction or label is no-answer, then f1=1 if they match, 0 otherwise\n","  if len(pred_tokens) == 0 or len(label_tokens) == 0:\n","    return int(pred_tokens==label_tokens)\n","\n","  commen_tokens = set(pred_tokens) & set(label_tokens) # & means overlap\n","  if len(commen_tokens) == 0:\n","    return 0\n","\n","  precision = len(commen_tokens) / len(pred_tokens)\n","  recall = len(commen_tokens) / len(label_tokens)\n","  f1 = (2*precision*recall) / (precision+recall)\n","  return f1\n","\n","def metric_max_over_label(metric_type, prediction, labels):\n","  scores = []\n","  for label in labels:\n","    score = metric_type(prediction, label)\n","    scores.append(score)\n","  return max(scores)\n","\n","def evaluate(gold_labels, predictions):\n","  f1 = e_m = total = 0\n","  for prediction,labels in zip(predictions,gold_labels):\n","    total += 1\n","    e_m += metric_max_over_label(metric_exact_match,prediction,labels)\n","    f1 += metric_max_over_label(metric_f1,prediction,labels)\n","  \n","  e_m = 100.0 * round(e_m/total,4)\n","  f1 = 100.0 * round(f1/total,4)\n","\n","  return e_m, f1\n","\n","def output_predictions_results(pipeline, language, path_predictions): \n","    \"\"\"\n","    extract gold labels from the test set, and get predictions using pipeline;\n","    evaluate and output the predictions;\n","    output the evaluation results to make qualitative analysis\n","    \"\"\"\n","    dataset_test = load_dataset(\"xquad\",f\"xquad.{language}\") #e.g. \"xquad.en\"\n","    cqa = [[],[],[]]\n","    for piece in dataset_test[\"validation\"]:\n","        cqa[0].append(piece[\"context\"])\n","        cqa[1].append(piece[\"question\"])\n","        cqa[2].append(piece[\"answers\"][\"text\"]) #xquad has only one answer\n","        assert len(cqa[0])==len(cqa[1])==len(cqa[2])\n","\n","    predictions = []\n","    for (c,q,a) in zip(cqa[0],cqa[1],cqa[2]):\n","        input = {\"context\":c, \"question\": q}\n","        prediction = pipeline(input,align_to_words=True)\n","        score = prediction[\"score\"]\n","        answer = prediction[\"answer\"]\n","        predictions.append(answer)\n","\n","        if path_predictions:    # qualitative analysis\n","            with open(path_predictions,\"a\") as f:\n","                f.write(f\"{c}\\t{q}\\t{a}\\t{answer}\\t{score}\\n\")\n","    \n","    return evaluate(cqa[2],predictions)"],"metadata":{"id":"na_E_dHN8Q2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","qa_ppl = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)\n","l_list = [\"en\",\"ar\",\"de\",\"th\",\"tr\",\"zh\"]\n","for l in l_list:\n","  language = l\n","  em, f1 = output_predictions_results(qa_ppl,language,path_predictions=None)\n","  tuning_method = \"finetune\" #or \"adapter\"/\"prefix\"\n","\n","  with open(\"./results_QA.txt\",\"a\") as f:\n","    # f.write(f\"model\\tlanguage\\tEM\\tF1\\n\")\n","    f.write(f\"{tuning_method}\\t{language}\\t{em}\\t{f1}\\n\")"],"metadata":{"id":"69V1MFEo8l6j"},"execution_count":null,"outputs":[]}]}