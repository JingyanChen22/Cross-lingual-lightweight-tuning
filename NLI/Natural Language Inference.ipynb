{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["4WzTJpGJW9mA","DMSHYALeOT09","8kD5r_-dXLVE","5FLrS-KZCrZi"],"toc_visible":true,"authorship_tag":"ABX9TyPEZ0gId/UEcRZt4As5igoc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1eopmApOqgU8AFJ0--5QmMnBqf-u29QGA?usp=sharing)"],"metadata":{"id":"0QDbMJxEA2ao"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojGeO5pWAuJ-"},"outputs":[],"source":["!pip install -U adapter-transformers\n","!pip install -U datasets\n","!pip install sentencepiece"]},{"cell_type":"markdown","source":["# model and tokenizer"],"metadata":{"id":"4WzTJpGJW9mA"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import torch\n","model_name = \"xlm-roberta-base\"\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=3).to(device)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"],"metadata":{"id":"3cWRXwR4XBuj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# data processing"],"metadata":{"id":"DMSHYALeOT09"}},{"cell_type":"code","source":["from datasets import load_dataset\n","dataset_valid = load_dataset('glue', 'mnli',split=\"validation_mismatched\")\n","dataset_train = load_dataset('glue', 'mnli',split=\"train\")"],"metadata":{"id":"DhS9LszPORXa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize_function(examples):\n","  encoded = tokenizer(examples[\"premise\"],examples[\"hypothesis\"],padding=\"max_length\",max_length=128, truncation=True)\n","  return {\"input_ids\": encoded[\"input_ids\"],\n","      \"attention_mask\": encoded[\"attention_mask\"],\n","      \"labels\":examples[\"label\"]\n","  }\n","\n","tokenized_valid = dataset_valid.map(\n","    tokenize_function, \n","    batched=True, \n","    batch_size=128,\n","    remove_columns=dataset_valid.column_names,\n",")\n","\n","tokenized_train = dataset_train.map(\n","    tokenize_function, \n","    batched=True, \n","    batch_size=128,\n","    remove_columns=dataset_train.column_names,\n",")"],"metadata":{"id":"xQe9LM6OdFQn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# set adapters/prefix"],"metadata":{"id":"8kD5r_-dXLVE"}},{"cell_type":"markdown","source":["## adapters"],"metadata":{"id":"1kI8Oq_lChI5"}},{"cell_type":"code","source":["from transformers import AdapterConfig\n","from transformers.adapters.composition import Stack\n","lang_adapter_config = AdapterConfig.load(\"pfeiffer\", reduction_factor=2)\n","model.load_adapter(\"en/wiki@ukp\", config=lang_adapter_config)\n","\n","adap_name = \"mnli\"\n","model.add_adapter(adap_name)\n","model.train_adapter(adap_name)\n","\n","model.active_adapters = Stack(\"en\",adap_name)"],"metadata":{"id":"pK3pfSuQW_v0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"With adapter parameters to train:\")\n","print(sum(p.numel() for p in model.parameters() if p.requires_grad))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u_GI8kyxCTph","executionInfo":{"status":"ok","timestamp":1679871841122,"user_tz":-120,"elapsed":452,"user":{"displayName":"Jingyan Chen","userId":"08159604947673382978"}},"outputId":"e32b9f07-893d-4f67-ee76-0fe267818272"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["With adapter parameters to train:\n","1487427\n"]}]},{"cell_type":"markdown","source":["## prefix"],"metadata":{"id":"jmqtRIw-Cisd"}},{"cell_type":"code","source":["from transformers.adapters import PrefixTuningConfig\n","\n","config = PrefixTuningConfig(flat=False, prefix_length=20)\n","model.add_adapter(\"prefix_tuning\", config=config)\n","\n","model.train_adapter(\"prefix_tuning\")\n","model.active_adapters = \"prefix_tuning\""],"metadata":{"id":"a52-hu7pCfGT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"With prefix parameters to train:\")\n","print(sum(p.numel() for p in model.parameters() if p.requires_grad))"],"metadata":{"id":"BO_EQx2iCmxX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# training"],"metadata":{"id":"XYMuWNw5C5Z_"}},{"cell_type":"code","source":["from transformers import TrainingArguments,Trainer,AdapterTrainer\n","from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n","\n","batch_size = 16\n","\n","training_args = TrainingArguments(\n","  output_dir = \"./training_nli\",\n","  log_level = \"error\",\n","  num_train_epochs = 1,\n","  learning_rate = 3e-5,\n","  lr_scheduler_type = \"linear\",\n","  warmup_ratio = 0.06,\n","  per_device_train_batch_size = batch_size,\n","  per_device_eval_batch_size = batch_size,\n","  adam_beta1 = 0.9,\n","  adam_beta2 = 0.999,\n","  adam_epsilon = 1e-8,\n","  evaluation_strategy = \"steps\",\n","  eval_steps = 5000, \n","  save_steps = 8000, \n","  logging_steps = 1000,\n","  save_total_limit=1,\n",")\n","\n","\n","trainer = AdapterTrainer( #using ``Trainer`` for full fine-tuning\n","  model = model,\n","  args = training_args,\n","  data_collator = data_collator,\n","  train_dataset = tokenized_train,\n","  eval_dataset = tokenized_valid,\n","  tokenizer = tokenizer,\n",")"],"metadata":{"id":"cHIq7J1ICW8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"wF0nY_wSDsKI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save lightweight tuned model\n","save_path = \"./NLI_adapter\" #or \"./NLI_prefix\"\n","os.makedirs(save_path, exist_ok=True)\n","trainer.save_model(save_path) "],"metadata":{"id":"0LwpQEjWLwkz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save full fine-tuned model\n","save_path = \"./NLI_finetune\"\n","os.makedirs(save_path, exist_ok=True)\n","trainer.model.save_pretrained(save_path)"],"metadata":{"id":"Oueo9lSaCFW2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# evaluation"],"metadata":{"id":"5FLrS-KZCrZi"}},{"cell_type":"markdown","source":["## load trained model"],"metadata":{"id":"niteZRC2Dv8S"}},{"cell_type":"code","source":["# full fine-tuned model\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import torch\n","import datasets\n","\n","model_name = \"xlm-roberta-base\"\n","model_path = \"./NLI_finetune\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device)"],"metadata":{"id":"CAV_GWRsEL0M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# adapter\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification,AdapterConfig\n","import torch\n","from datasets import load_dataset\n","import datasets\n","from transformers.adapters.composition import Stack\n","\n","model_name = \"xlm-roberta-base\"\n","model_path = \"./NLI_adapter\"\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=3)\n","model.load_adapter(f\"{model_path}/mnli\")\n","\n","lang_adapter_config = AdapterConfig.load(\"pfeiffer\", reduction_factor=2)\n","model.load_adapter(\"en/wiki@ukp\", config=lang_adapter_config)\n","model.active_adapters = Stack(\"en\",\"mnli\")\n","model.to(device)"],"metadata":{"id":"An2H8n3hD668"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prefix\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import torch\n","import datasets\n","\n","model_name = \"xlm-roberta-base\"\n","model_path = \"./NLI_prefix\"\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=3).to(device)\n","\n","model.load_adapter(f\"{model_path}/prefix_tuning\")\n","model.active_adapters = \"prefix_tuning\"\n","model.to(device)"],"metadata":{"id":"07YXkXf9DySO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## output results"],"metadata":{"id":"img5igRdCtAI"}},{"cell_type":"code","source":["import datasets\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\n","data_type = \"cross_test\" #or \"mix_test\"/\"in_test\"\n","language_list = [\"en\",\"ar\",\"de\",\"th\",\"tr\",\"zh\"]\n","\n","for language in language_list:\n","  if data_type == \"mix_test\":\n","    test_path = f\"./NLI_test_data/{data_type}/{language}\"\n","    test_dataset = datasets.load_from_disk(test_path)\n","    premises = test_dataset['premise']\n","    hypotheses = test_dataset['hypothesis']\n","    labels = test_dataset['label']\n","  else:\n","    test_dataset = pd.read_csv(f\"./NLI_test_data/{data_type}/{language}.csv\")\n","    premises = test_dataset['sentence1']\n","    hypotheses = test_dataset['sentence2']\n","    labels_text = test_dataset[\"gold_label\"]\n","    label_map = {\"contradiction\": 2, \"neutral\": 1, \"entailment\": 0}\n","    labels = [label_map[label] for label in labels_text]\n","\n","  encoded_dict = tokenizer.batch_encode_plus(\n","      list(zip(premises, hypotheses)),\n","      add_special_tokens=True,\n","      padding=True,\n","      truncation=True,\n","      return_attention_mask=True,\n","      return_tensors='pt'\n","  )\n","\n","  input_ids = encoded_dict['input_ids']\n","  attention_masks = encoded_dict['attention_mask']\n","\n","  from torch.utils.data import TensorDataset, DataLoader\n","  dataset = TensorDataset(input_ids, attention_masks, torch.tensor(labels))\n","  dataloader = DataLoader(dataset, batch_size=8)\n","  \n","  model.eval()\n","\n","  predictions = []\n","  true_labels = []\n","\n","  with torch.no_grad():\n","    for batch in dataloader:\n","      batch = tuple(t.to(device) for t in batch)\n","      input_ids, attention_masks, labels = batch\n","\n","      outputs = model(input_ids, attention_mask=attention_masks)\n","      _, pred = torch.max(outputs[0], dim=1)\n","\n","      predictions.extend(pred.cpu().numpy())\n","      true_labels.extend(labels.cpu().numpy())\n","  accuracy = round(accuracy_score(true_labels, predictions)*100,2)\n","  f1 = round(f1_score(true_labels, predictions, average=\"weighted\")*100,2)\n","  mcc = round(matthews_corrcoef(true_labels, predictions)*100, 2)\n","\n","  tuning_method = \"finetune\" #or \"adapter\"/\"prefix\"\n","\n","  with open(\"./results_NLI.txt\", \"a\") as f:\n","    f.write(f\"{tuning_method}\\t{language}\\t{f1}\\t{accuracy}\\t{mcc}\\t{data_type}\\n\")\n","  print(f\"F1: {f1}\\nAcc: {accuracy}\\nMcc: {mcc}\")"],"metadata":{"id":"8CdN6-c6C2yl"},"execution_count":null,"outputs":[]}]}